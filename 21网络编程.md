# 2.1.1 网络io与select, poll,epoll
## socket 与文件描述符的关联
### socket 函数
- 功能： 指定协议类型
- 定义： 
```c
	#include<sys/types.h>
	#include<sys/socket.h>
	int socket(int family, int type, int protocol);
	- return value
	- error: -1;
	- success: 套接字 sockfd;
	- 理解socket
		socket使用 Unix 文件描述符 (file descriptor) 和其他程序通讯的方式。Unix 程序在执行任何形式的 I/O 的时候，程序是在读或者写一个文件描述符。一个文件描述符只是一个和打开的文件相关联的整数。这个文件可能是一个网络连接，FIFO，管道，终端，磁盘上的文件或者什么其他的东西。Unix 中所有的东西是文件！因此，与 Internet 上别的程序通讯的时候，要通过文件描述符。利用系统调用 socket()得到网络通讯的文件描述符。他返回套接口描述符 (socket descriptor)，然后再通过他来调用 send() 和 recv()。
```

## sigio 的异步通知

- 在前面使用阻塞或者非阻塞的方式来读取驱动中按键值都是应用程序主动读取的，对于非阻塞方式来说还需要应用程序通过 poll 函数不断的轮询。最好的方式就是驱动程序能主动向应用程序发出通知，报告自己可以访问，然后应用程序在从驱动程序中读取或写入数据，类似于我们在裸机例程中讲解的中断。Linux 提供了异步通知这个机制来完成此功能
- 中断是处理器提供的一种异步机制，我们配置好中断以后就可以让处理器去处理其他的事情了，当中断发生以后会触发我们事先设置好的中断服务函数，在中断服务函数中做具体的处理。比如我们在裸机篇里面编写的 GPIO 按键中断实验，我们通过按键去开关蜂鸣器，采用中断以后处理器就不需要时刻的去查看按键有没有被按下，因为按键按下以后会自动触发中断。同样的，Linux 应用程序可以通过阻塞或者非阻塞这两种方式来访问驱动设备，通过阻塞方式访问的话应用程序会处于休眠态，等待驱动设备可以使用，非阻塞方式的话会通过 poll 函数来不断的轮询，查看驱动设备文件是否可以使用。这两种方式都需要应用程序主动的去查询设备的使用情况，如果能提供一种类似中断的机制，当驱动程序可以访问的时候主动告诉应用程序那就最好了。
  “信号”为此应运而生，信号类似于我们硬件上使用的“中断”，只不过信号是软件层次上的。算是在软件层次上对中断的一种模拟，驱动可以通过主动向应用程序发送信号的方式来报告自己可以访问了，应用程序获取到信号以后就可以从驱动设备中读取或者写入数据了。整个过程就相当于应用程序收到了驱动发送过来了的一个中断，然后应用程序去响应这个中断，在整个处理过程中应用程序并没有去查询驱动设备是否可以访问，一切都是由驱动设备自己告诉给应用程序的。
  阻塞、非阻塞、异步通知，这三种是针对不同的场合提出来的不同的解决方法，没有优劣之分，在实际的工作和学习中，根据自己的实际需求选择合适的处理方法即可。

### signal 函数
```c
	#include<signal.h>
	#include<sys/types.h>
	sighandler_t signal(int signum, sighandler_t handler)
	- args
	- sgnum: 要设置处理函数的信号；
	- handler: 信号的处理函数
	- return value: 
		- error: SIG_ERR
		- success: 返回信号的前一个处理函数
```
### 应用程序对异步通知的处理
- 注册信号处理函数
  应用程序根据驱动程序所使用的信号设置信号的处理函数，应用程序使用signal 函数来设置信号的处理函数。
- 将本应用程序的进程号告诉内核
  使用fcntl(fd, F_SETOWN, getpid()) 将本应用程序的进程号告诉内核
- 开启异步通知
  使用如下两行程序开启异步通知
```c
flags = fcntl(fd, F_GETFL);  // 获取当前的进程状态
fcntl(fd, F_SETFL, flags | FASYNC);  // 开启当前进程异步通知功能
```

## 多路复用select/poll

- select，poll，epoll都是IO多路复用的机制。所谓I/O多路复用机制，就是说通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。
- select和poll的实现比较相似，目前也有很多为人诟病的缺点，epoll可以说是select和poll的增强版。

### 一、select实现

1. 使用copy_from_user从用户空间拷贝fd_set到内核空间
2. 注册回调函数__pollwait
3. 遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）
4. 以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。
5. __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。
6. poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。
7. 如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。
8. 把fd_set从内核空间拷贝到用户空间。

**总结：**
select的几大缺点：

1. 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
2. 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
3. select支持的文件描述符数量太小了，默认是1024

### 二、poll实现

poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构。其他的都差不多。

### 三、epoll实现

epoll既然是对select和poll的改进，就应该能避免上述的三个缺点。那epoll都是怎么解决的呢？在此之前，我们先看一下epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。
对于**第一个缺点**，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。
对于**第二个缺点**，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链

可以看到，总体和select的实现是类似的，只不过它是创建了一个eppoll_entry结构pwq，只不过pwq->wait的func成员被设置成了回调函数ep_poll_callback（而不是default_wake_function，所以这里并不会有唤醒操作，而只是执行回调函数），private成员被设置成了NULL。最后吧pwq->wait链入到whead中（也就是设备等待队列中）。这样，当设备等待队列中的进程被唤醒时，就会调用ep_poll_callback了。

再梳理一下，当epoll_wait时，它会判断就绪链表中有没有就绪的fd，如果没有，则把current进程加入一个等待队列(file->private_data->wq)中，并在一个while（1）循环中判断就绪队列是否为空，并结合schedule_timeout实现睡一会，判断一会的效果。如果current进程在睡眠中，设备就绪了，就会调用回调函数。在回调函数中，会把就绪的fd放入就绪链表，并唤醒等待队列(file->private_data->wq)中的current进程，这样epoll_wait又能继续执行下去了。
对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。
**总结：**

1. select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。
2. select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。


| selec | poll        | epoll      |                                     |
| ----- | ----------- | ---------- | ----------------------------------- |
| 数据结构  | bitmap      | 数组         | 红黑树                                 |
| 最大连接数 | 1024        | 无上限        | 无上限                                 |
| fd拷贝  | 每次调用selec拷贝 | 每次调用poll拷贝 | fd首次调用epoll_ctl拷贝，每次调用epoll_wait不拷贝 |
| 工作效率  | 轮询O：(n）     | 轮询：O(n）    | 回调：O(1）                             |

## 手撕epoll 单线程，多线程，多进程的多种写法
### 单线程
#### 函数分析
```c
#include<sys/epoll.h>
epoll_create(int size);
- size: 可忽略大小

epoll_ctl(int epfd, int op, int fd, struct epoll_event * event);
- epfd: 用于注册监视对象的epoll例程的文件描述符
- op: 指定监视对象的添加、删除或更改等操作
- fd: 需要注册的监视对象的文件描述符
- event: 监视对象的时间类型

epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout);
- epfd: epoll例程的文件描述符
- events: 保存发生事件的文件描述符集合的结构体地址值，其所指缓冲需要动态分配
- maxevents: 第二个参数中可以保存的最大事件数
- timeout:  是以1/1000秒为单位的等待时间，传递-1时，一直等待直到发生事件
```
- epoll_ctl 第二个参数传递的常量及含义：

    EPOLL_CTL_ADD：将文件描述符注册到epoll例程

    EPOLL_CTL_DEL：从epoll例程中删除文件描述符（此时第四个参数传递NULL，但Linux 2.6.9之前不行）

    EPOLL_CTL_MOD：更改注册的文件描述符的关注事件发生情况

- epoll_event中的成员events中可以保存的常量及其所指的事件类型：

    EPOLLIN：需要读取数据的情况

    EPOLLOUT：输出缓冲为空，可以立即发送数据的情况

    EPOLLPRI：收到OOB数据的情况

    EPOLLDHUP：断开连接或半关闭的情况，这在边缘触发方式下非常有用
    EPOLLERR：发生错误的情况

    EPOLLET：以边缘触发的方式得到事件通知

    EPOLLONESHOT：发生一次事件后，相应文件描述符不再收到事件通知。因此需要向epoll_ctl函数的第二个参数传递

	EPOLL_CTL_MOD，再次设置事件
#### code
```c
/* 基于epoll的回声服务器 */
#include<stdio.h>
#include<unistd.h>
#include<stdlib.h>
#include<sys/epoll.h>
#include<arpa/inet.h>
#include<signal.h>
#include<fcntl.h>
#include<errno.h>

#define MAX_EVENTS 10
#define SERVER_PORT 9999
#define SERVER_ADDR INADDR_ANY
#define EPOLL_SIZE 0

void setnoblocking(int fd){
    int flags = fcntl(fd, F_GETFL, 0);
    flags |= O_NONBLOCK;
    int res = fcntl(fd, F_SETFL, flags);
    if(res < 0){
        perror("fcrnl");
        exit(EXIT_FAILURE);
    }
}

int main(){

    struct epoll_event ev, events[MAX_EVENTS];
    int listen_sock, conn_sock, nfds;

    listen_sock = socket(AF_INET, SOCK_STREAM, 0);
    if(listen_sock == -1){
        perror("socket");
        exit(EXIT_FAILURE);
    }

    struct sockaddr_in saddr;
    saddr.sin_family = AF_INET;
    saddr.sin_port = htons(SERVER_PORT);
    saddr.sin_addr.s_addr = htonl(SERVER_ADDR);

    int ret = bind(listen_sock, (struct sockaddr *)&saddr, sizeof(saddr));
    if(ret == -1){
        perror("bind");
        exit(EXIT_FAILURE);
    }

    ret = listen(listen_sock, 128);
    if(ret == -1){
        perror("listen");
        exit(EXIT_FAILURE);
    }

    int efd = epoll_create1(EPOLL_SIZE);
    if(efd == -1){
        perror("epoll_create");
        exit(EXIT_FAILURE);
    }    

    //events = malloc(sizeof(struct epoll_event) * EPOLL_SIZE);


    ev.events = EPOLLIN;
    ev.data.fd = listen_sock;
    if(epoll_ctl(efd, EPOLL_CTL_ADD, listen_sock, &ev) == -1){
        perror("epoll_ctl");
        exit(EXIT_FAILURE);
    }

    for(;;){
        struct sockaddr_in addr;
        socklen_t addrlen = sizeof(addr);
        char recvbuf[128];
        
        nfds = epoll_wait(efd, events, MAX_EVENTS, -1);
        if(nfds == -1){
            perror("epoll_wait");
            exit(EXIT_FAILURE);
        }
        int i;
        for(i = 0; i < nfds; i++){
            if(events[i].data.fd == listen_sock){
                conn_sock = accept(listen_sock,
                                (struct sockaddr *)&addr, &addrlen);
                if(conn_sock == -1){
                    perror("accept");
                    exit(EXIT_FAILURE);
                }
                setnoblocking(conn_sock);
                ev.events = EPOLLIN | EPOLLET;
                ev.data.fd = conn_sock;
                if(epoll_ctl(efd, EPOLL_CTL_ADD, conn_sock, &ev) == -1){
                    perror("epoll_ctl: conn_sock");
                    exit(EXIT_FAILURE);
                }
                printf("connected client: %d\n",conn_sock);
            }else{
                // do_use_fd(events[i].data.fd);

                int len = read(events[i].data.fd, recvbuf, sizeof(recvbuf));
                if(len == 0){
                    epoll_ctl(efd, EPOLL_CTL_DEL, events[i].data.fd, &ev);
                    close(events[i].data.fd);
                    printf("closed client: %d\n", events[i].data.fd);
                    break;
                }
                else if(len < 0){
                    if(errno == EAGAIN){
                        break;
                    }
                }
                else {
                    write(events[i].data.fd, recvbuf, len);
                }
            }
        }
    }
    close(listen_sock);
    close(efd);
    return 0;
}
```
```c
/* 实现边缘触发的服务器 */
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <fcntl.h>
#include <errno.h>

#define BUF_SIZE 4
#define EPOLL_SIZE 50
void setnonblockingmode(int fd);
void error_handling(char *buf);

int main(int argc, char *argv[])
{
    int serv_sock, clnt_sock;
    struct sockaddr_in serv_adr, clnt_adr;
    socklen_t adr_sz;
    int str_len, i;
    char buf[BUF_SIZE];
    
    struct epoll_event *ep_events;
    struct epoll_event event;
    int epfd, event_cnt;
    
    if (argc != 2)
    {
        printf("Usage : %s <port>\n", argv[0]);
        exit(1);
    }
    
    serv_sock = socket(PF_INET, SOCK_STREAM, 0);
    memset(&serv_adr, 0, sizeof(serv_adr));
    serv_adr.sin_family = AF_INET;
    serv_adr.sin_addr.s_addr = htonl(INADDR_ANY);
    serv_adr.sin_port = htons(atoi(argv[1]));

    if (bind(serv_sock, (struct sockaddr*)&serv_adr, sizeof(serv_adr)) == -1)
    {
        error_handling("bind() error");
    }
    if (listen(serv_sock, 5) == -1)
    {
        error_handling("listen() error");
    }

    epfd = epoll_create(EPOLL_SIZE);                                                      // 创建例程
    ep_events = malloc(sizeof(struct epoll_event) * EPOLL_SIZE);

    setnonblockingmode(serv_sock);
    event.events = EPOLLIN;
    event.data.fd = serv_sock;
    epoll_ctl(epfd, EPOLL_CTL_ADD, serv_sock, &event);                                    // 注册服务端

    while (1)
    {
        event_cnt = epoll_wait(epfd, ep_events, EPOLL_SIZE, -1);
        if (event_cnt == -1)
        {
            puts("epoll_wait() error");
            break;
        }

        puts("return epoll_wait");
        for (i = 0; i < event_cnt; i++)
        {
            if (ep_events[i].data.fd == serv_sock)                                        // 说明有客户端连接
            {
                adr_sz = sizeof(clnt_adr);
                clnt_sock = accept(serv_sock, (struct sockaddr*)&clnt_adr, &adr_sz);
                setnonblockingmode(clnt_sock);                                            // 将套接字改为非阻塞模式
                event.events = EPOLLIN|EPOLLET;                                          // 将事件注册方式改为边缘触发
                event.data.fd = clnt_sock;
                epoll_ctl(epfd, EPOLL_CTL_ADD, clnt_sock, &event);                        // 注册客户端
                printf("connected client: %d \n", clnt_sock);
            }
            else
            {
                while(1)
                {
                    str_len = read(ep_events[i].data.fd, buf, BUF_SIZE);
                    if (str_len == 0)                                                         // 数据接收完毕
                    {
                        epoll_ctl(epfd, EPOLL_CTL_DEL, ep_events[i].data.fd, NULL);
                        close(ep_events[i].data.fd);
                        printf("closed client: %d \n", ep_events[i].data.fd);
                        break;
                    }
                    else if (str_len < 0)
                    {
                        if (errno == EAGAIN)                                                 // 读取了输入缓冲中的全部数据
                        {
                            break;
                        }
                    }
                    else
                    {
                        write(ep_events[i].data.fd, buf, str_len);
                }
                }
            }
        }
    }
    close(serv_sock);
    close(epfd);
    return 0;
}

void setnonblockingmode(int fd)
{
    int flag = fcntl(fd, F_GETFL, 0);
    fcntl(fd, F_SETFL, flag|O_NONBLOCK);
}
void error_handling(char *buf)
{
    fputs(buf, stderr);
    fputc('\n', stderr);
    exit(1);
}
```


## 代码实现 LT/ET 的区别
```c
/* 
    略微优化的epoll
    可以选择ET/LT
*/
#include<sys/types.h>
#include<sys/socket.h>
#include<unistd.h>
#include<assert.h>
#include<stdio.h>
#include<string.h>
#include<stdlib.h>
#include<netinet/in.h>
#include<errno.h>
#include<arpa/inet.h>
#include<fcntl.h>
#include<sys/epoll.h>
#include<pthread.h>

#define MAX_EVENT_NUMBER 1024
#define BUFFER_SIZE 10
#define BOOL int
#define TRUE 1
#define FALSE 0

/* 将文件描述符设置成非阻塞的 */
int setnonblocking(int fd)
{
	int old_option = fcntl(fd, F_GETFL);
	int new_option = old_option | O_NONBLOCK;
	fcntl(fd, F_SETFL, new_option);
	return old_option;
}

/* 将文件描述符 fd 上的 EPOLLIN 注册到 epollfd 指示的 epoll 内核事件表中，参数 enable_et 指定是否对 fd 启用 ET 模式 */
void addfd(int epollfd, int fd, BOOL enable_et)
{
	struct epoll_event event;
	event.data.fd = fd;
	event.events = EPOLLIN;
	if (enable_et)
	{
		event.events |= EPOLLET;
	}
	epoll_ctl(epollfd, EPOLL_CTL_ADD, fd, &event);
	setnonblocking(fd);
}

/* LT 模式工作流程 */
void lt(struct epoll_event* events, int number, int epollfd, int listenfd)
{
	char buf[BUFFER_SIZE];
	for (int i = 0; i < number; i++)
	{
		int sockfd = events[i].data.fd;
		if (sockfd == listenfd)
		{
			struct sockaddr_in client_address;
			socklen_t client_addrlength = sizeof(client_address);
			int connfd = accept(listenfd, (struct sockaddr*)&client_address, &client_addrlength);
			addfd(epollfd, connfd, FALSE);	/* 把 connfd 添加到内核事件表中，并禁用 ET 模式 */
		}
		else if (events[i].events & EPOLLIN)
		{
			/* 只要 socket 读缓存中还有未读出的数据，这段代码就被触发 */
			printf("EPOLLIN event trigger once\n");
			memset(buf, '\0', BUFFER_SIZE);
			int ret = recv(sockfd, buf, BUFFER_SIZE - 1, 0);
			if (ret <= 0)
			{
				close(sockfd);
				continue;
			}
			printf("get %d bytes of content: %s\n", ret, buf);
		}
		else
		{
			printf("something else happened \n");
		}
	}
}

/* ET 模式的工作流程 */
void et(struct epoll_event* events, int number, int epollfd, int listenfd)
{
	char buf[BUFFER_SIZE];
	for (int i = 0; i < number; i++)
	{
		int sockfd = events[i].data.fd;
		if (sockfd == listenfd)
		{
			struct sockaddr_in client_address;
			socklen_t client_addrlength = sizeof(client_address);
			int connfd = accept(sockfd, (struct sockaddr*)&client_address, &client_addrlength);
			addfd(epollfd, connfd, TRUE);	/* 把 connfd 添加到内核事件表中，并开启 ET 模式 */
		}
		else if (events[i].events & EPOLLIN)
		{
			/* 这段代码不会被重复触发，所以必须循环读取数据，以确保把 socket 读缓存中的所有数据读出 */
			printf("EPOLLIN event trigger once\n");
			while (1)
			{
				memset(buf, '\0', BUFFER_SIZE);
				int ret = recv(sockfd, buf, BUFFER_SIZE - 1, 0);
				if (ret < 0)
				{
					/* 对于非阻塞IO，下面的条件成立表示数据已全部读取完毕。此后，epoll就能再次触发 sockfd 上的 EPOLLIN 事件，进行下一次读操作*/
					if (errno == EAGAIN || errno == EWOULDBLOCK)
					{
						printf("raed later\n");
						break;
					}
					close(sockfd);
					break;
				}
				else if (ret == 0)	/* 返回0，表示对方已经关闭连接了 */
				{
					close(sockfd);
				}
				else 
				{
					printf("get %d bytes of content: %s\n", ret, buf);
				}
			}
		}
		else
		{
			printf("something else happened \n");
		}
	}
}

int main(int argc, char* argv[])
{
	if (argc <= 2)
	{
		return 1;
	}
	const char* ip = argv[1];
	int port = atoi(argv[2]);
	
	int ret = 0;
	struct sockaddr_in address;
	bzero(&address, sizeof(address));
	address.sin_family = AF_INET;
	address.sin_port = htons(port);
	inet_pton(AF_INET, ip, &address.sin_addr);

	int listenfd = socket(PF_INET, SOCK_STREAM, 0);
	assert(listenfd >= 0);

	ret = bind(listenfd, (struct sockaddr*)&address, sizeof(address));
	assert(ret != -1);

	ret = listen(listenfd, 5);
	assert(ret != -1);

	struct epoll_event events[MAX_EVENT_NUMBER];
	int epollfd = epoll_create(10);
	assert(epollfd != -1);
	addfd(epollfd, listenfd, TRUE);

	while (1)
	{
		int ret = epoll_wait(epollfd, events, MAX_EVENT_NUMBER, -1);
		if (ret < 0)
		{
			printf("epoll filure\n");
			break;
		}

		// lt(events, ret, epollfd, listenfd);
		et(events, ret, epollfd, listenfd);
	}

	close(listenfd);
	return 0;
}

```

# 2.1.2 reactor 的原理与实现

## 原理

在处理web请求时，通常有两种体系结构，分别为：**thread-based architecture（基于线程）**、**event-driven architecture（事件驱动）**

**thread-based architecture**

基于线程的体系结构通常会使用多线程来处理客户端的请求，每当接收到一个请求，便开启一个独立的线程来处理。这种方式虽然是直观的，但是仅适用于并发访问量不大的场景，因为线程需要占用一定的内存资源，且操作系统在线程之间的切换也需要一定的开销，当线程数过多时显然会降低web服务器的性能。并且，当线程在处理I/O操作，在等待输入的这段时间线程处于空闲的状态，同样也会造成cpu资源的浪费。
**code**
```c
#include <stdio.h>
#include <sys/socket.h>
#include <sys/types.h>
#include <netinet/in.h>
#include <fcntl.h>
#include <unistd.h>
#include <pthread.h>
#define BUFFER_LENGTH	128

// thread --> fd
void *routine(void *arg) {
	int clientfd = *(int *)arg;
	while (1) {
		unsigned char buffer[BUFFER_LENGTH] = {0};
		int ret = recv(clientfd, buffer, BUFFER_LENGTH, 0);
		if (ret == 0) {
			close(clientfd);
			break;
		}
		printf("buffer : %s, ret: %d\n", buffer, ret);
		ret = send(clientfd, buffer, ret, 0); // 
	}
}

int main() 
{
	// 创建监听fd
	int listenfd = socket(AF_INET, SOCK_STREAM, 0);  // 
	if (listenfd == -1) return -1;
	
	struct sockaddr_in servaddr;
	servaddr.sin_family = AF_INET;
	servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
	servaddr.sin_port = htons(9999);
	
	// 绑定
	if (-1 == bind(listenfd, (struct sockaddr*)&servaddr, sizeof(servaddr))) {
		return -2;
	}
	// 设置listenfd为非阻塞模式
	// int flag = fcntl(listenfd, F_GETFL, 0);
	// flag |= O_NONBLOCK;
	// fcntl(listenfd, F_SETFL, flag);

	listen(listenfd, 10); // 默认listenfd为阻塞模式

	while (1) {
		struct sockaddr_in client;
		socklen_t len = sizeof(client);
		// 阻塞等待客户端连接上来
		int clientfd = accept(listenfd, (struct sockaddr*)&client, &len);
		
		pthread_t threadid;
		// 为连接上来的客户端fd，创建线程，并叫该客户端fd作为参数传入，线程处理函数
		pthread_create(&threadid, NULL, routine, &clientfd);
	}

	return 0;
}
```

**event-driven architecture**

事件驱动体系结构是目前比较广泛使用的一种。这种方式会定义一系列的事件处理器来响应事件的发生，并且将服务端接受连接与对事件的处理分离。其中，事件是一种状态的改变。比如，tcp中socket的new incoming connection、ready for read、ready for write。
### reactor
![](./reactor1.png)
reactor主要由以下几个角色构成：handle、Synchronous Event Demultiplexer、Initiation Dispatcher、Event Handler、Concrete Event Handler
**handle**
handle在linux中一般称为文件描述符，而在window称为句柄，两者的含义一样。handle是事件的发源地。比如一个网络socket、磁盘文件等。而发生在handle上的事件可以有connection、ready for read、ready for write等。
**Synchronous Event Demultiplexer**
同步事件分离器，本质上是系统调用。比如linux中的select、poll、epoll等。比如，select方法会一直阻塞直到handle上有事件发生时才会返回。
**Event Handler**
事件处理器，其会定义一些回调方法或者称为钩子函数，当handle上有事件发生时，回调方法便会执行，一种事件处理机制。
**Concrete Event Handler**
具体的事件处理器，实现了Event Handler。在回调方法中会实现具体的业务逻辑。
**Initiation Dispatcher**
初始分发器，也是reactor角色，提供了注册、删除与转发event handler的方法。当Synchronous Event Demultiplexer检测到handle上有事件发生时，便会通知initiation dispatcher调用特定的event handler的回调方法。
**code**
```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>

#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <time.h>

#define BUFFER_LENGTH		4096
#define MAX_EPOLL_EVENTS	1024
#define SERVER_PORT			8888
#define PORT_COUNT			1

typedef int NCALLBACK(int ,int, void*);
int recv_cb(int fd, int events, void *arg);
int send_cb(int fd, int events, void *arg);
struct ntyevent *ntyreactor_idx(struct ntyreactor *reactor, int sockfd);

struct ntyevent {
	int fd;
	int events;
	void *arg;
	int (*callback)(int fd, int events, void *arg);
	int status;
	char buffer[BUFFER_LENGTH];
	int length;
	long last_active;
};

struct eventblock {
	struct eventblock *next;
	struct ntyevent *events;
};

struct ntyreactor {
	int epfd;
	int blkcnt;
	struct eventblock *evblk; //fd --> 100w
};

// 初始化ntyevent对象（设置fd，绑定回调，给回调传参等）
void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK callback, void *arg) {
	ev->fd = fd;
	ev->callback = callback;
	ev->events = 0;
	ev->arg = arg;
	ev->last_active = time(NULL);
	return ;	
}


int nty_event_add(int epfd, int events, struct ntyevent *ev) {
	struct epoll_event ep_ev = {0, {0}};
	ep_ev.data.ptr = ev;
	ep_ev.events = ev->events = events;

	int op;
	if (ev->status == 1) {
		op = EPOLL_CTL_MOD;
	} else {
		op = EPOLL_CTL_ADD;
		ev->status = 1;
	}
	if (epoll_ctl(epfd, op, ev->fd, &ep_ev) < 0) {
		printf("event add failed [fd=%d], events[%d]\n", ev->fd, events);
		return -1;
	}
	return 0;
}

// 将ntyevent对象从 epoll集合中删除
int nty_event_del(int epfd, struct ntyevent *ev) {
	struct epoll_event ep_ev = {0, {0}};
	if (ev->status != 1) {
		return -1;
	}
	ep_ev.data.ptr = ev;
	ev->status = 0;
	epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &ep_ev);
	return 0;
}

int recv_cb(int fd, int events, void *arg) {

	struct ntyreactor *reactor = (struct ntyreactor*)arg;
	struct ntyevent *ev = ntyreactor_idx(reactor, fd);

	int len = recv(fd, ev->buffer, BUFFER_LENGTH , 0); // 
	nty_event_del(reactor->epfd, ev);   // 删除该对象的读事件

	if (len > 0) {
		ev->length = len;
		ev->buffer[len] = '\0';
		printf("C[%d]:%s\n", fd, ev->buffer)
		nty_event_set(ev, fd, send_cb, reactor);
		nty_event_add(reactor->epfd, EPOLLOUT, ev);  // 加入该对象的写事件
	} else if (len == 0) {
		close(ev->fd);
		//printf("[fd=%d] pos[%ld], closed\n", fd, ev-reactor->events);
	} else {
		close(ev->fd);
		printf("recv[fd=%d] error[%d]:%s\n", fd, errno, strerror(errno));
	}
	return len;
}

int send_cb(int fd, int events, void *arg) {
	struct ntyreactor *reactor = (struct ntyreactor*)arg;
	struct ntyevent *ev = ntyreactor_idx(reactor, fd);

	// 填充完数据返回给客户端
	int len = send(fd, ev->buffer, ev->length, 0);
	if (len > 0) {
		printf("send[fd=%d], [%d]%s\n", fd, len, ev->buffer);

		nty_event_del(reactor->epfd, ev);     // 删除写事件
		nty_event_set(ev, fd, recv_cb, reactor);
		nty_event_add(reactor->epfd, EPOLLIN, ev);  // 加入读事件
		
	} else {
		close(ev->fd);
		nty_event_del(reactor->epfd, ev);
		printf("send[fd=%d] error %s\n", fd, strerror(errno));
	}
	return len;
}

// 将连接上来的client fd，创建ntyevent对象，并绑定recv_cb，加入到epoll集合
int accept_cb(int fd, int events, void *arg) {
	struct ntyreactor *reactor = (struct ntyreactor*)arg;
	if (reactor == NULL) return -1;

	struct sockaddr_in client_addr;
	socklen_t len = sizeof(client_addr);

	int clientfd;
	if ((clientfd = accept(fd, (struct sockaddr*)&client_addr, &len)) == -1) {
		if (errno != EAGAIN && errno != EINTR) {
			
		}
		printf("accept: %s\n", strerror(errno));
		return -1;
	}
	// int flag = 0;
	// if ((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0) {
	// 	printf("%s: fcntl nonblocking failed, %d\n", __func__, MAX_EPOLL_EVENTS);
	// 	return -1;
	// }
	struct ntyevent *event = ntyreactor_idx(reactor, clientfd);
	nty_event_set(event, clientfd, recv_cb, reactor);
	nty_event_add(reactor->epfd, EPOLLIN, event);
	printf("new connect [%s:%d], pos[%d]\n", 
		inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);
	return 0;
}

int init_sock(short port) {

	int fd = socket(AF_INET, SOCK_STREAM, 0);
	fcntl(fd, F_SETFL, O_NONBLOCK);

	struct sockaddr_in server_addr;
	memset(&server_addr, 0, sizeof(server_addr));
	server_addr.sin_family = AF_INET;
	server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
	server_addr.sin_port = htons(port);

	bind(fd, (struct sockaddr*)&server_addr, sizeof(server_addr));

	if (listen(fd, 20) < 0) {
		printf("listen failed : %s\n", strerror(errno));
	}

	return fd;
}


int ntyreactor_alloc(struct ntyreactor *reactor) {

	if (reactor == NULL) return -1;
	if (reactor->evblk == NULL) return -1;

	struct eventblock *blk = reactor->evblk;
	while (blk->next != NULL) {
		blk = blk->next;
	}

	struct ntyevent *evs = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
	if (evs == NULL) {
		printf("ntyreactor_alloc ntyevents failed\n");
		return -2;
	}
	memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));

	struct eventblock *block = (struct eventblock *)malloc(sizeof(struct eventblock));
	if (block == NULL) {
		printf("ntyreactor_alloc eventblock failed\n");
		return -2;
	}
	memset(block, 0, sizeof(struct eventblock));

	block->events = evs;
	block->next = NULL;

	blk->next = block;
	reactor->blkcnt ++; //

	return 0;
}

struct ntyevent *ntyreactor_idx(struct ntyreactor *reactor, int sockfd) {
	int blkidx = sockfd / MAX_EPOLL_EVENTS;
	while (blkidx >= reactor->blkcnt) {
		ntyreactor_alloc(reactor);
	}
	struct eventblock *blk = reactor->evblk;

	int i = 0;
	while(i ++ < blkidx && blk != NULL) {
		blk = blk->next;
	}
	return &blk->events[sockfd % MAX_EPOLL_EVENTS];
}


int ntyreactor_init(struct ntyreactor *reactor) {
	if (reactor == NULL) return -1;
	memset(reactor, 0, sizeof(struct ntyreactor));

	// 1. 创建epoll fd
	reactor->epfd = epoll_create(1);
	if (reactor->epfd <= 0) {
		printf("create epfd in %s err %s\n", __func__, strerror(errno));
		return -2;
	}

	// 2.创建一个event对象
	struct ntyevent *evs = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
	if (evs == NULL) {
		printf("ntyreactor_alloc ntyevents failed\n");
		return -2;
	}
	memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));

	// 3.初始化链表第一个block
	struct eventblock *block = (struct eventblock *)malloc(sizeof(struct eventblock));
	if (block == NULL) {
		printf("ntyreactor_alloc eventblock failed\n");
		return -2;
	}
	memset(block, 0, sizeof(struct eventblock));

	block->events = evs;
	block->next = NULL;

	reactor->evblk = block;
	reactor->blkcnt = 1;

	return 0;
}

int ntyreactor_destory(struct ntyreactor *reactor) {

	close(reactor->epfd);
	//free(reactor->events);

	struct eventblock *blk = reactor->evblk;
	struct eventblock *blk_next = NULL;

	while (blk != NULL) {
		blk_next = blk->next;
		free(blk->events);
		free(blk);
		blk = blk_next;
	}
	return 0;
}


// 将多个服务器socket，加入到epoll集合，
int ntyreactor_addlistener(struct ntyreactor *reactor, int sockfd, NCALLBACK *acceptor) {
	if (reactor == NULL) return -1;
	if (reactor->evblk == NULL) return -1;

	//reactor->evblk->events[sockfd];
	struct ntyevent *event = ntyreactor_idx(reactor, sockfd);

	nty_event_set(event, sockfd, acceptor, reactor);

	nty_event_add(reactor->epfd, EPOLLIN, event);
	return 0;
}

int ntyreactor_run(struct ntyreactor *reactor) {
	if (reactor == NULL) return -1;
	if (reactor->epfd < 0) return -1;
	if (reactor->evblk == NULL) return -1;
	struct epoll_event events[MAX_EPOLL_EVENTS+1];
	
	int checkpos = 0, i;

	while (1) {

		int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, -1);
		if (nready < 0) {
			printf("epoll_wait error, exit\n");
			continue;
		}

		for (i = 0;i < nready;i ++) {
			struct ntyevent *ev = (struct ntyevent*)events[i].data.ptr;
			printf("current ready fd = %d\n", ev->fd);
			
			//   回调函数在这里被调用
			if ((events[i].events & EPOLLIN) && (ev->events & EPOLLIN)) {
				ev->callback(ev->fd, events[i].events, ev->arg);
			}
			if ((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT)) {
				ev->callback(ev->fd, events[i].events, ev->arg);
			}	
		}
	}
}

// 3, 6w, 1, 100 == 
// <remoteip, remoteport, localip, localport>

int main(int argc, char *argv[]) {
	unsigned short port = SERVER_PORT; // listen 8888
	if (argc == 2) {
		port = atoi(argv[1]);
	}
	struct ntyreactor *reactor = (struct ntyreactor*)malloc(sizeof(struct ntyreactor));
	ntyreactor_init(reactor);

	int i = 0;
	int sockfds[PORT_COUNT] = {0};
	for (i = 0;i < PORT_COUNT;i ++) {
		sockfds[i] = init_sock(port+i);
		printf("listen fd = %d\n", sockfds[i]);
		ntyreactor_addlistener(reactor, sockfds[i], accept_cb);
	}

	ntyreactor_run(reactor);
	ntyreactor_destory(reactor);

	for (i = 0;i < PORT_COUNT;i ++) {
		close(sockfds[i]);
	}
	free(reactor);

	return 0;
```

#### 处理流程
1. 当应用向Initiation Dispatcher注册Concrete Event Handler时，应用会标识出该事件处理器希望Initiation Dispatcher在某种类型的事件发生发生时向其通知，事件与handle关联

2. Initiation Dispatcher要求注册在其上面的Concrete Event Handler传递内部关联的handle，该handle会向操作系统标识

3. 当所有的Concrete Event Handler都注册到 Initiation Dispatcher上后，应用会调用handle_events方法来启动Initiation Dispatcher的事件循环，这时Initiation Dispatcher会将每个Concrete Event Handler关联的handle合并，并使用Synchronous Event Demultiplexer来等待这些handle上事件的发生

4. 当与某个事件源对应的handle变为ready时，Synchronous Event Demultiplexer便会通知 Initiation Dispatcher。比如tcp的socket变为ready for reading

5. Initiation Dispatcher会触发事件处理器的回调方法。当事件发生时， Initiation Dispatcher会将被一个“key”（表示一个激活的handle）定位和分发给特定的Event Handler的回调方法

6. Initiation Dispatcher调用特定的Concrete Event Handler的回调方法来响应其关联的handle上发生的事件


## reactor 针对业务实现的优点

1. 响应快，不必为单个同步事件所阻塞，虽然Reactor本身依然是同步的
2. 可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程的切换开销

3. 扩展性好，可以方便的通过增加Reactor实例个数来充分利用CPU资源

4. 复用性好，Reactor模型本身与具体事件处理逻辑无关，具有很高的复用性
   ​

## epoll 封装send_cb/recv_cb/accept_cb
## reactor 多核实现
## 跨平台(select/epoll/kqueue)的封装reactor